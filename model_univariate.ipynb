{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import tensorflow_probability as tfp\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "from numpy import split\n",
    "import random\n",
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n",
    "from scipy.optimize import minimize\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import numpy\n",
    "from tqdm import tqdm_notebook\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import array\n",
    "#from itertools import product\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Layer\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from keras.layers import TimeDistributed,BatchNormalization\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from numpy.random import seed\n",
    "from scipy import stats\n",
    "from IPython.display import clear_output\n",
    "import statistics\n",
    "\n",
    "import keras.backend as K\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from scipy.stats import loguniform, uniform, randint\n",
    "import ast\n",
    "\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# evaluation mertic (symmteric mean absolute percentage error)\n",
    "def smape(yTrue,yPred):\n",
    "  smape=0\n",
    "\n",
    "  for i in range(len(yTrue)):\n",
    "    smape+= abs(yTrue[i]-yPred[i])/ (abs(yTrue[i])+abs(yPred[i]))\n",
    "  smape/=len(yTrue)\n",
    "\n",
    "  return smape\n",
    "\n",
    "\n",
    "# Dynamic Penalty Function - Calculate IQR-based penalty\n",
    "def calculate_dynamic_iqr_penalty(actual, mean_predictions):\n",
    "    \"\"\"Calculate dynamic penalty based on IQR.\"\"\"\n",
    "    iqr_actual = np.percentile(actual, 75) - np.percentile(actual, 25)\n",
    "    iqr_pred = np.percentile(mean_predictions, 75) - np.percentile(mean_predictions, 25)\n",
    "    \n",
    "    # Define the penalty dynamically based on the difference in IQRs\n",
    "    if iqr_pred < iqr_actual:\n",
    "        iqr_ratio = iqr_pred / iqr_actual\n",
    "        penalty = (1 - iqr_ratio) * 0.3  # Penalty is proportional to IQR difference\n",
    "        print(f'**** PENALIZED: +{penalty:.4f} ******')\n",
    "        return penalty  # Return only the penalty\n",
    "    else:\n",
    "        return 0  # No penalty if predicted IQR is sufficient\n",
    "\n",
    "\n",
    "def exponential_smoothing(series, alpha):\n",
    "    smoothed_series = [series[0]]\n",
    "    for i in range(1, len(series)):\n",
    "        smoothed_series.append(alpha * series[i] + (1 - alpha) * smoothed_series[i-1])\n",
    "    return smoothed_series\n",
    "\n",
    "def des_smoothing(series, alpha, beta):\n",
    "    \"\"\"\n",
    "    Applies Double Exponential Smoothing (DES) to a time series for both smoothing and forecasting.\n",
    "\n",
    "    Parameters:\n",
    "    series (list or array-like): The input time series data.\n",
    "    alpha (float): Smoothing factor for the level (between 0 and 1).\n",
    "    beta (float): Smoothing factor for the trend (between 0 and 1).\n",
    "\n",
    "    Returns:\n",
    "    list: The smoothed series with forecasted values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the smoothed series with the first value\n",
    "    smoothed_series = [series[0]]\n",
    "\n",
    "    for i in range(1, len(series) + 1):\n",
    "        if i == 1:\n",
    "            # Initial level and trend setup\n",
    "            level = series[0]\n",
    "            trend = series[1] - series[0]\n",
    "        if i >= len(series):\n",
    "            # If we're forecasting beyond the data, use the last smoothed value\n",
    "            observation = smoothed_series[-1]\n",
    "        else:\n",
    "            # Otherwise, use the actual data\n",
    "            observation = series[i]\n",
    "\n",
    "        # Store the previous level for trend calculation\n",
    "        previous_level = level\n",
    "\n",
    "        # Update the level and trend using the smoothing factors\n",
    "        level = alpha * observation + (1 - alpha) * (level + trend)\n",
    "        trend = beta * (level - previous_level) + (1 - beta) * trend\n",
    "\n",
    "        # Append the new smoothed value (level + trend)\n",
    "        smoothed_series.append(level + trend)\n",
    "\n",
    "    return smoothed_series\n",
    "\n",
    "def determine_optimal_lag(series, max_lags,lags=36):\n",
    "    \"\"\"\n",
    "    Determine the optimal lag for a time series using ACF.\n",
    "\n",
    "    Parameters:\n",
    "    series (array-like): The input time series data.\n",
    "    max_lags (int): The maximum number of lags to consider.\n",
    "\n",
    "    Returns:\n",
    "    int: The optimal lag to use for the model.\n",
    "    \"\"\"\n",
    "    # Calculate ACF and PACF values\n",
    "    acf_values = acf(series, nlags=lags)\n",
    "    # pacf_values = pacf(series, nlags=lags)\n",
    "    \n",
    "    # Determine significance level\n",
    "    significance_level = 1.96 / np.sqrt(len(series))\n",
    "    \n",
    "    # Find the first lag where ACF and PACF drop below the significance level\n",
    "    optimal_lag_acf = np.where(np.abs(acf_values) < significance_level)[0]\n",
    "    # optimal_lag_pacf = np.where(np.abs(pacf_values) < significance_level)[0]\n",
    "    \n",
    "    # Take the first occurrence or default to max_lags if not found\n",
    "    optimal_lag_acf = optimal_lag_acf[0] if len(optimal_lag_acf) > 1 else max_lags\n",
    "    # optimal_lag_pacf = optimal_lag_pacf[0] if len(optimal_lag_pacf) > 1 else max_lags\n",
    "    \n",
    "    # Select the maximum of ACF and PACF lags\n",
    "    optimal_lag = min(optimal_lag_acf, max_lags)\n",
    "    \n",
    "    # Plot ACF and PACF for inspection\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    plot_acf(series, lags=lags, ax=ax)\n",
    "    ax.set_title('Autocorrelation Function (ACF)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Optimal Lag: {optimal_lag}\")\n",
    "    return optimal_lag\n",
    "\n",
    "\n",
    "def create_multivariate_lagged_dataset(series, n_lags):\n",
    "    \"\"\"\n",
    "    Create a dataset where each time step t uses the previous n_lags time steps of all features as inputs.\n",
    "\n",
    "    Parameters:\n",
    "    series (array-like): The input time series data with multiple features (shape: [num_samples, num_features]).\n",
    "    n_lags (int): The number of previous time steps to use as inputs.\n",
    "\n",
    "    Returns:\n",
    "    X (numpy array): Input features of shape (num_samples, n_lags, num_features).\n",
    "    y (numpy array): Output labels of shape (num_samples, num_features).\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "\n",
    "    # Ensure the series is a numpy array\n",
    "    series = np.array(series)\n",
    "\n",
    "    if series.ndim == 1:\n",
    "      series = series.reshape(-1, 1)\n",
    "\n",
    "    # Loop over the series and create input-output pairs\n",
    "    for i in range(n_lags, len(series[:,0])):\n",
    "        X.append(series[i-n_lags:i, :])  # The previous n_lags time steps for all features\n",
    "        y.append(series[i, :])           # The current time step for all features\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "#Bayesian LSTM\n",
    "class BayesianLSTM(LSTM): #inherits LSTM but set training=True which keeps dropout active during inference phase as well.\n",
    "\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(BayesianLSTM, self).__init__(units, **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(BayesianLSTM, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        # Call the parent class LSTM with training set to True\n",
    "        return super(BayesianLSTM, self).call(inputs, training=True, **kwargs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_sequences:\n",
    "            return (input_shape[0], input_shape[1], self.units)\n",
    "        else:\n",
    "            return (input_shape[0], self.units)\n",
    "\n",
    "\n",
    "class TemporalAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TemporalAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='temporal_attention_weight', shape=(input_shape[-1], input_shape[-1]), initializer='glorot_uniform', trainable=True)\n",
    "        self.b = self.add_weight(name='temporal_attention_bias', shape=(input_shape[-1],), initializer='zeros', trainable=True)\n",
    "        self.s = self.add_weight(name='importance_score', shape=(input_shape[-1],), initializer='glorot_uniform', trainable=True)\n",
    "        super(TemporalAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Using TensorFlow operations\n",
    "        uit = tf.tanh(tf.tensordot(x, self.W, axes=1) + self.b)\n",
    "        ait = tf.tensordot(uit, self.s, axes=1)  # tenosr dot operation on time step axis leve\n",
    "        ait = tf.nn.softmax(ait, axis=1)\n",
    "        ait = tf.expand_dims(ait, axis=-1)\n",
    "        weighted_input = x * ait\n",
    "        return tf.reduce_sum(weighted_input, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train the model using Bayesian LSTM using Dual Stage Attention mechanism\n",
    "def build_model(data, n_input, layer, unit, epoch, lr, rdo, n_features,l2_reg,attack_title):\n",
    "    # Prepare data\n",
    "    train_x, train_y = create_multivariate_lagged_dataset(data, n_input)\n",
    "    \n",
    "    # Define model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add LSTM and Attention Mechanism based on the number of layers\n",
    "    if layer == 1:\n",
    "        # First LSTM Layer with L2 Regularization\n",
    "        model.add(BayesianLSTM(unit[0], input_shape=(n_input, n_features), \n",
    "                               recurrent_dropout=rdo, return_sequences=True, \n",
    "                               activation=keras.layers.LeakyReLU(alpha=0.2),\n",
    "                               kernel_regularizer=l2(l2_reg)))\n",
    "        \n",
    "        # Batch Normalization after LSTM layer\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        # Temporal Attention Mechanism applied to the hidden states\n",
    "        model.add(TemporalAttention())\n",
    "        model.add(RepeatVector(1))\n",
    "\n",
    "        # Second LSTM Layer (Decoder) with L2 Regularization\n",
    "        model.add(BayesianLSTM(unit[0], activation=keras.layers.LeakyReLU(alpha=0.2), \n",
    "                               return_sequences=True, recurrent_dropout=rdo,\n",
    "                               kernel_regularizer=l2(l2_reg)))\n",
    "        \n",
    "        # Batch Normalization after Decoder LSTM\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    elif layer == 2:\n",
    "        # First LSTM Layer with L2 Regularization\n",
    "        model.add(BayesianLSTM(unit[0], input_shape=(n_input, n_features), \n",
    "                               recurrent_dropout=rdo, return_sequences=True, \n",
    "                               activation=keras.layers.LeakyReLU(alpha=0.2),\n",
    "                               kernel_regularizer=l2(l2_reg)))\n",
    "        \n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        # Second encoder LSTM Layer with L2 Regularization\n",
    "        model.add(BayesianLSTM(unit[1], activation=keras.layers.LeakyReLU(alpha=0.2), \n",
    "                               return_sequences=True, recurrent_dropout=rdo,\n",
    "                               kernel_regularizer=l2(l2_reg)))\n",
    "        \n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        # Temporal Attention Mechanism applied to the hidden states\n",
    "        model.add(TemporalAttention())\n",
    "        model.add(RepeatVector(1))\n",
    "\n",
    "        # Decoder LSTM first Layers\n",
    "        model.add(BayesianLSTM(unit[1], activation=keras.layers.LeakyReLU(alpha=0.2), \n",
    "                               return_sequences=True, recurrent_dropout=rdo,\n",
    "                               kernel_regularizer=l2(l2_reg)))\n",
    "        \n",
    "        # Batch Normalization after first Decoder LSTM\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(BayesianLSTM(unit[0], activation=keras.layers.LeakyReLU(alpha=0.2), \n",
    "                               return_sequences=True, recurrent_dropout=rdo,\n",
    "                               kernel_regularizer=l2(l2_reg)))\n",
    "        #Decoder LSTM second Layers\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    # Output Layers with L2 Regularization (Common for both conditions)\n",
    "    model.add(TimeDistributed(Dense(100, activation=keras.layers.LeakyReLU(alpha=0.2), \n",
    "                                    kernel_regularizer=l2(l2_reg))))\n",
    "    model.add(TimeDistributed(Dense(n_features, kernel_regularizer=l2(l2_reg))))\n",
    "\n",
    "    # Compile the model\n",
    "    opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=opt, loss='mse', metrics=['mape'])\n",
    "\n",
    "    # Early stopping\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n",
    "\n",
    "    # Fit the model\n",
    "    print(train_x.shape)\n",
    "    train_history = model.fit(train_x, train_y, epochs=epoch, batch_size=8, \n",
    "                              validation_split=0.3, verbose=0, callbacks=[es])\n",
    "    \n",
    "    plt.plot(train_history.history['loss'], label='Training Loss')\n",
    "    plt.plot(train_history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"Training_Results/Univariate/{attack_title}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Return the model and training history\n",
    "    return model\n",
    "\n",
    "def scale_data(data):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    #Normalising the columns\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    return scaled_data\n",
    "\n",
    "def smooth_and_plot_data(data, alpha,attack, n_features,beta=1):\n",
    "    # Split the data into training and testing\n",
    "    featureList=[]\n",
    "    plt.clf()\n",
    "    if not alpha==1 and not beta==1:\n",
    "        smoothed= des_smoothing(data[:,0], alpha,beta)\n",
    "        smoothed=smoothed[0:-1] #remove the one future step forecast from des\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(data[:, 0], label=\"Actual\", color='blue', linestyle='-', linewidth=2)\n",
    "        plt.plot(smoothed, label=f\"Smoothed (α={alpha}, β={beta})\", color=\"red\", linestyle='--', linewidth=2)\n",
    "        plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "        plt.title(f\"Double Exponential Smoothing - {attack}\", fontsize=16, fontweight='bold')\n",
    "        plt.xlabel(\"Month\", fontsize=12)\n",
    "        if 'Mentions-' in attack:\n",
    "            plt.ylabel(\"No of mentions\")\n",
    "        else:\n",
    "            mention_as_feature= True\n",
    "            plt.ylabel(\"No of incidents\")\n",
    "        # Highlighting the max point in the smoothed data\n",
    "        max_val, min_val = np.max(smoothed), np.min(smoothed)\n",
    "        max_index, min_index = np.argmax(smoothed), np.argmin(smoothed)\n",
    "        plt.scatter(max_index, max_val, color='green', s=50, label=f'Max Value: {max_val:.2f}')\n",
    "        plt.scatter(min_index, min_val, color='pink', s=50, label=f'Min Value: {min_val:.2f}')\n",
    "        plt.tight_layout()\n",
    "        plt.legend(loc=\"best\", fontsize=12)\n",
    "        plt.show() #plot the data and smoothed data\n",
    "        featureList.append(smoothed)\n",
    "\n",
    "    elif not alpha==1 and beta==1:\n",
    "        smoothed= exponential_smoothing(data[:,0], alpha)\n",
    "        smoothed=smoothed[0:-1] #remove the one future step forecast from des\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(data[:, 0], label=\"Actual\", color='blue', linestyle='-', linewidth=2)\n",
    "        plt.plot(smoothed, label=f\"Smoothed (α={alpha}, β={beta})\", color=\"red\", linestyle='--', linewidth=2)\n",
    "        plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "        plt.title(f\"Exponential Smoothing - {attack}\", fontsize=16, fontweight='bold')\n",
    "        plt.xlabel(\"Month\", fontsize=12)\n",
    "        if 'Mentions-' in attack:\n",
    "            plt.ylabel(\"No of mentions\")\n",
    "        else:\n",
    "            mention_as_feature= True\n",
    "            plt.ylabel(\"No of incidents\")\n",
    "        # Highlighting the max point in the smoothed data\n",
    "        max_val, min_val = np.max(smoothed), np.min(smoothed)\n",
    "        max_index, min_index = np.argmax(smoothed), np.argmin(smoothed)\n",
    "        plt.scatter(max_index, max_val, color='green', s=50, label=f'Max Value: {max_val:.2f}')\n",
    "        plt.scatter(min_index, min_val, color='pink', s=50, label=f'Min Value: {min_val:.2f}')\n",
    "        plt.tight_layout()\n",
    "        plt.legend(loc=\"best\", fontsize=12)\n",
    " \n",
    "\n",
    "        plt.show() #plot the data and smoothed data\n",
    "        featureList.append(smoothed)\n",
    "    else:\n",
    "        return data    \n",
    "    featureList=numpy.transpose(featureList)\n",
    "    prepared_data=numpy.array(featureList).reshape(-1,n_features)\n",
    "    return prepared_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_test_split(data, n_input):\n",
    "  split_position = len(data) - (36 + n_input)\n",
    "  if split_position <= 0:\n",
    "    raise ValueError(\"n_input is too large for the dataset size.\")\n",
    "  train, test = data[:split_position], data[split_position:]\n",
    "  return train,test\n",
    "\n",
    "\n",
    "a= 0\n",
    "\n",
    "def model_evaluation(data, attack, param_distributions, attack_title,n_iter, best_smape):\n",
    "    \"\"\"\n",
    "    Uses RandomizedSearchCV to evaluate the model on different hyperparameter combination.\n",
    "\n",
    "    Parameters:\n",
    "    data: The input data for evaluation.\n",
    "    attack: The target column to focus on.\n",
    "    param_distributions: Dictionary of hyperparameter distributions for RandomizedSearchCV.\n",
    "    n_iter: Number of random selections for RandomizedSearchCV..\n",
    "    best_smape: Best Smape value to start with and then later on changed based on result\n",
    "\n",
    "    Returns:\n",
    "    best_model: The best performing model based on the mean SMAPE.\n",
    "    best_params: The best hyperparameters.\n",
    "    best_mape: The lowest average SMAPE achieved.\n",
    "    \"\"\"\n",
    "\n",
    "    class BayesianLSTMEstimator(BaseEstimator, RegressorMixin):\n",
    "        def __init__(self, alpha, beta, unit, epoch, lr, rdo, n_features, attack,l2_reg):\n",
    "            self.alpha = alpha\n",
    "            self.beta = beta\n",
    "            self.n_input = None\n",
    "            self.unit = unit\n",
    "            self.epoch = epoch\n",
    "            self.lr = lr\n",
    "            self.rdo = rdo\n",
    "            self.n_features = n_features\n",
    "            self.attack = attack\n",
    "            self.train,self.test=None,None\n",
    "            self.scaler=None\n",
    "            self.l2_reg=l2_reg\n",
    "            self.scaled_data = None\n",
    "            self.relevant_scaled_data=None\n",
    "\n",
    "        def fit(self, X, y=None):\n",
    "\n",
    "            global a\n",
    "            seed(a+1)\n",
    "            tf.random.set_seed(a+1)\n",
    "            print(f'--------------Starting the iteration {a+1}---------------')\n",
    "            print(f'Selected Parameters: {self.get_params()}')\n",
    "            print('--------------------------------------------------------------------------------')\n",
    "            print(f\"Smoothing with alpha={self.alpha}, beta={self.beta} and splitting with n_input={self.n_input}\")\n",
    "            smoothed_data = smooth_and_plot_data(X, self.alpha, self.attack, self.n_features, self.beta)\n",
    "            self.scaled_data = scale_data(smoothed_data)\n",
    "            print('data shape=',self.scaled_data.shape)\n",
    "            e_index=0 #emerging index\n",
    "            for i in range(len(self.scaled_data[:,0])-2):\n",
    "                if self.scaled_data[i,0]>0 and self.scaled_data[i+1,0]>0 and self.scaled_data[i+2,0]>0:\n",
    "                    e_index=i\n",
    "                    break\n",
    "            print('Emerging Index=',e_index)\n",
    "\n",
    "            self.relevant_scaled_data=self.scaled_data[e_index:,]\n",
    "            print('Length of data after cut=',len(self.relevant_scaled_data[:,0]))\n",
    "            max_lags = (len(self.relevant_scaled_data)//10)+1\n",
    "            self.n_input = determine_optimal_lag(self.relevant_scaled_data[:,0],max_lags,36)\n",
    "            self.train, self.test = train_test_split(self.relevant_scaled_data, self.n_input)\n",
    "            # Build and fit the model\n",
    "            self.model = build_model(self.relevant_scaled_data, self.n_input, len(self.unit), self.unit, self.epoch, self.lr, self.rdo, self.n_features,self.l2_reg,attack_title)\n",
    "            return self\n",
    "        def predict(self, X, y=None):\n",
    "          \"\"\"\n",
    "          Perform multiple iterations of predictions and calculate the mean,\n",
    "          along with lower and upper bounds for uncertainty estimation.\n",
    "          \"\"\"\n",
    "          n_iterations = 10  # Number of iterations for Monte Carlo sampling\n",
    "          all_predictions = []\n",
    "          initial_history = self.test[:self.n_input]  # Initial history for generating future predictions\n",
    "\n",
    "          # Perform n_iterations of forecasting\n",
    "          for iteration in range(n_iterations):\n",
    "              history = initial_history.copy()\n",
    "              predictions = []\n",
    "\n",
    "              # Rolling window prediction for the test set\n",
    "              for _ in range(len(self.test) - self.n_input):\n",
    "                  y_pred = self.model.predict(history.reshape(1, history.shape[0], history.shape[1]))\n",
    "                  next_value = y_pred[0][0]  # Extract the forecasted value\n",
    "                  predictions.append(next_value)\n",
    "\n",
    "                  # Update the history by appending the new prediction\n",
    "                  history = np.append(history, next_value).reshape(-1, self.n_features)[1:]\n",
    "\n",
    "              all_predictions.append(predictions)\n",
    "\n",
    "          # Convert to numpy array for statistical analysis\n",
    "          all_predictions = np.array(all_predictions)\n",
    "\n",
    "          # Compute the mean, and the 95% confidence intervals (2.5th and 97.5th percentiles)\n",
    "          mean_forecast = np.mean(all_predictions, axis=0)\n",
    "          lower_bound = np.percentile(all_predictions, 2.5, axis=0)\n",
    "          upper_bound = np.percentile(all_predictions, 97.5, axis=0)\n",
    "          return mean_forecast[:,0], lower_bound[:,0], upper_bound[:,0]\n",
    "\n",
    "\n",
    "        def score(self, X, y=None):\n",
    "            \"\"\"\n",
    "            Train and evaluate the model from scratch multiple times (as specified by repeats).\n",
    "            The average SMAPE is returned as the score.\n",
    "            \"\"\"\n",
    "            global a\n",
    "            a=a+1\n",
    "            print(f\"Selected Parameters: {self.get_params()}\")\n",
    "            mean_predictions, lower_bound, upper_bound = self.predict(X)\n",
    "            smape_score = smape(self.test[self.n_input:,], mean_predictions)\n",
    "            # Apply dynamic IQR penalty\n",
    "            penalty = calculate_dynamic_iqr_penalty(self.test[self.n_input:, ], mean_predictions)\n",
    "            final_score = smape_score + penalty  # Apply the penalty to SMAPE\n",
    "        \n",
    "            print(f\"Initial SMAPE = {smape_score}, Penalty = {penalty},  Corrected SMAPE= {final_score}\")\n",
    "            # Plot prediction\n",
    "            global best_smape\n",
    "            self.plot_predictions(mean_predictions, lower_bound, upper_bound,final_score,best_smape)\n",
    "            if final_score < best_smape:\n",
    "                best_smape = final_score\n",
    "\n",
    "            print(f'Best SMAPE uptill now: {best_smape}')               \n",
    "            print(f\"Current Iteration SMAPE = {final_score}\")\n",
    "            print(f'-----------End of Iteration {a}--------------------------')\n",
    "            return -final_score  # Negative because RandomizedSearchCV maximizes the score, but we want to minimize SMAPE\n",
    "\n",
    "        def plot_predictions(self, mean_predictions, lower_bound, upper_bound,smape,best_smape):\n",
    "            \"\"\"\n",
    "            Plot actual values, mean predictions, and 95% confidence intervals for the current repeat.\n",
    "\n",
    "            Parameters:\n",
    "            mean_predictions: Mean predictions for the current repeat.\n",
    "            lower_bound: Lower bound of the 95% confidence interval.\n",
    "            upper_bound: Upper bound of the 95% confidence interval.\n",
    "            \"\"\"\n",
    "            actual = self.test[self.n_input:, ]\n",
    "            x = range(1, len(mean_predictions) + 1)\n",
    "            plt.figure(figsize=(10, 6), facecolor='white')\n",
    "            plt.plot(x, actual, label='Actual', color='#1f77b4', linestyle='-', linewidth=2.5, marker='o', markersize=6, markerfacecolor='blue')\n",
    "            plt.plot(x, mean_predictions, label='Predicted', color='#ff7f0e', linestyle='--', linewidth=2.5, marker='D', markersize=6, markerfacecolor='orange')\n",
    "            plt.fill_between(x, lower_bound, upper_bound, color='lightgreen', alpha=0.3, label='95% Confidence Interval')\n",
    "            plt.grid(True, which='both', color='gray', linestyle='--', linewidth=0.6, alpha=0.7)\n",
    "            plt.legend(loc=\"best\", fontsize=12, fancybox=True, framealpha=1, shadow=True, borderpad=1)\n",
    "            plt.title(f'{attack}(Univariate), SMAPE: {smape}', fontsize=18, fontweight='bold', color='#2c3e50', pad=20)\n",
    "            plt.xlabel(\"Months\", fontsize=14, fontweight='bold', color='#34495e')\n",
    "            plt.ylabel(\"Number of Incidents\", fontsize=14, fontweight='bold', color='#34495e')\n",
    "            plt.xticks(fontsize=12, fontweight='bold', color='#2c3e50')\n",
    "            plt.yticks(fontsize=12, fontweight='bold', color='#2c3e50')\n",
    "            plt.tight_layout()\n",
    "            if smape < best_smape:\n",
    "                plt.savefig(f\"Evaluation Result/Univariate/{attack_title}\"+\".png\")\n",
    "            plt.show()\n",
    "            plt.gcf()\n",
    "\n",
    "    dummy_y = np.zeros(len(data))\n",
    "\n",
    "    # Initialize RandomizedSearchCV\n",
    "    model = BayesianLSTMEstimator(alpha=None, beta=None, unit=None, epoch=None, lr=None, rdo=None, n_features=None, attack=None,l2_reg=None)\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=n_iter,\n",
    "        cv=[(slice(None), slice(None))],  # No actual CV, just one pass\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit and find the best model\n",
    "    random_search.fit(data, dummy_y)\n",
    "\n",
    "    best_model = random_search.best_estimator_  # Best model based on the search\n",
    "    best_params = random_search.best_params_\n",
    "    best_mape = -random_search.best_score_  # Convert back to positive SMAPE\n",
    " \n",
    "    print(\"Best Parameters: \", best_params)\n",
    "    print(\"Best Average SMAPE: \", best_mape)\n",
    "\n",
    "    # After evaluation, plot and save the best model's forecast\n",
    "    best_predictions, best_lower_bound, best_upper_bound = best_model.predict(best_model.relevant_scaled_data)\n",
    "    best_model.plot_predictions(best_predictions, best_lower_bound, best_upper_bound,best_mape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return best_model, best_params, best_mape\n",
    "\n",
    "\n",
    "def forecast(data, best_params, attack_title ,repeat):\n",
    "    \"\"\"\n",
    "    Forecast the next 36 months using the best model from RandomizedSearchCV.\n",
    "    Perform 10 Monte Carlo iterations to account for uncertainty and calculate the mean forecast and confidence intervals.\n",
    "\n",
    "    Parameters:\n",
    "    - best_model: The best model returned from RandomizedSearchCV.\n",
    "    - best_params: Best hyperparameters returned from RandomizedSearchCV.\n",
    "    - data: Entire historical data(unscaled).\n",
    "    - attack_title: Title of the plot.\n",
    "    \"\"\"\n",
    "    alpha=float(best_params['alpha'])\n",
    "    beta=float(best_params['beta'])\n",
    "    unit= [int(x) for x in best_params['unit']]\n",
    "    epoch=int(best_params['epoch'])\n",
    "    lr=float(best_params['lr'])\n",
    "    rdo=float(best_params['rdo'])\n",
    "    n_features=int(best_params['n_features'])\n",
    "    attack=best_params['attack']\n",
    "    l2_reg=float(best_params['l2_reg']) \n",
    "\n",
    "\n",
    "    for i in range(repeat):\n",
    "        seed(i)\n",
    "        tf.random.set_seed(i) \n",
    "        # Initialize a list to store all predictions\n",
    "        all_predictions = []\n",
    "        smoothed_data = smooth_and_plot_data(data, alpha, attack, n_features,beta)\n",
    "        scaled_data = scale_data(smoothed_data)\n",
    "        print('data shape=',scaled_data.shape)\n",
    "        index=0 #emerging index\n",
    "        for i in range(len(scaled_data[:,0])-2):\n",
    "            if scaled_data[i,0]>0 and scaled_data[i+1,0]>0 and scaled_data[i+2,0]>0:\n",
    "                index=i\n",
    "                break\n",
    "        print('Emerging Index=',index)\n",
    "\n",
    "        relevant_scaled_data=scaled_data[index:,]\n",
    "        print('Length of data after cut=',len(relevant_scaled_data[:,0]))\n",
    "        max_lags = (len(relevant_scaled_data[:,0])//10)+1\n",
    "        n_input = determine_optimal_lag(relevant_scaled_data[:,0],max_lags,36)\n",
    "        model = build_model(relevant_scaled_data, n_input,len(unit), unit, epoch, lr,rdo,n_features,l2_reg,attack_title)\n",
    "        # Perform 10 Monte Carlo iterations of forecasting\n",
    "        for iteration in range(10):\n",
    "            history = relevant_scaled_data[-n_input:].copy()  # Start with the last n_input historical data points\n",
    "            predictions = []\n",
    "            \n",
    "            # Generate predictions for the next 36 months\n",
    "            for _ in range(36):\n",
    "                y_pred = model.predict(history.reshape(1, history.shape[0], history.shape[1]))\n",
    "                next_value = y_pred[0][0]  # Extract the forecasted value\n",
    "                predictions.append(next_value)\n",
    "                \n",
    "                # Update the history by appending the new prediction and removing the oldest value\n",
    "                history = np.append(history, next_value).reshape(-1, n_features)[1:]\n",
    "\n",
    "            all_predictions.append(predictions)\n",
    "        \n",
    "        # Convert to numpy array for statistical analysis\n",
    "        all_predictions = np.array(all_predictions)\n",
    "\n",
    "        # Compute the mean forecast and 95% confidence intervals (2.5th and 97.5th percentiles)\n",
    "        mean_forecasts = np.mean(all_predictions, axis=0)\n",
    "        lower_bound = np.percentile(all_predictions, 2.5, axis=0)\n",
    "        upper_bound = np.percentile(all_predictions, 97.5, axis=0)\n",
    "        \n",
    "        # Plot the forecast with historical data\n",
    "        plot_forecast(mean_forecasts[:,0], lower_bound[:,0], upper_bound[:,0], attack_title, scaled_data[:,0],i)\n",
    "\n",
    "        with open('Output Forecast/Univariate/'+ attack_title.replace('/','_')+f'_forecast_{i+1}.txt', 'w') as f:\n",
    "            f.write('Forecast: '+str(mean_forecasts[:,0].tolist()))\n",
    "            f.close()\n",
    "\n",
    "\n",
    "def plot_forecast(mean_forecasts, lower_bound, upper_bound, attack_title, historical_actual_data,a):\n",
    "    \"\"\"\n",
    "    Plot the forecast with 95% confidence intervals, including historical data,\n",
    "    from July 2011 to March 2024, followed by the forecast from April 2024 to March 2027.\n",
    "    The x-axis will display yearly labels based on the number of data points.\n",
    "\n",
    "    Parameters:\n",
    "    - mean_forecasts: Mean forecasts for the next 36 months (April 2024 to March 2027).\n",
    "    - lower_bound: Lower bound of the 95% confidence interval for the forecasts.\n",
    "    - upper_bound: Upper bound of the 95% confidence interval for the forecasts.\n",
    "    - attack_title: Title of the plot.\n",
    "    - historical_actual_data: Historical data (actual data) for the period from July 2011 to March 2024.\n",
    "    - a: seed\n",
    "    \"\"\"\n",
    "    total_data_points = len(historical_actual_data) + len(mean_forecasts)\n",
    "\n",
    "    # Generate year labels starting from 2012, placed at every 12th tick (i.e., 9, 21, 33,...)\n",
    "    years = ['2012'] + [str(2012 + (i // 12)) for i in range(12, total_data_points - 8, 12)]\n",
    "    print(years)\n",
    "\n",
    "    # Calculate the tick locations: starting from tick 9, every 12 ticks\n",
    "    ticks = range(9, total_data_points+1, 12)\n",
    "\n",
    "    # Set up the figure\n",
    "    plt.figure(figsize=(14, 7), facecolor='white')\n",
    "\n",
    "    # Plot historical data (July 2011 - March 2024)\n",
    "    plt.plot(range(len(historical_actual_data)), historical_actual_data, label='Historical Data', color='royalblue', linestyle='-', linewidth=2)\n",
    "\n",
    "    plt.plot(range(len(historical_actual_data) - 1, total_data_points), [historical_actual_data[-1]] + list(mean_forecasts), label='Forecast', color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "    # Fill the 95% confidence interval for the forecasted data\n",
    "    plt.fill_between(range(len(historical_actual_data), total_data_points), lower_bound, upper_bound, color='lightgreen', alpha=0.3, label='95% Confidence Interval')\n",
    "\n",
    "    # Add grid lines only at the major ticks (where the labels are)\n",
    "    plt.gca().set_xticks(ticks)  # Set major ticks only at labeled positions\n",
    "    plt.grid(True, which='major', color='lightgray', linestyle='--', linewidth=0.8, alpha=0.9)\n",
    "\n",
    "    # Add labels, title, and legend\n",
    "    plt.legend(loc=\"best\", fontsize=12, fancybox=True, framealpha=1, shadow=True, borderpad=1)\n",
    "    plt.title(f'{attack_title} - Historical Data & 36 Months Forecast(Univariate)', fontsize=18, fontweight='bold', color='#333333', pad=20)\n",
    "    plt.xlabel(\"Year\", fontsize=14, fontweight='bold', color='#333333')\n",
    "    plt.ylabel(\"Number of Incidents\", fontsize=14, fontweight='bold', color='#333333')\n",
    "\n",
    "    # Set x-axis ticks to match the year labels and apply label formatting\n",
    "    plt.xticks(ticks, years, rotation=45, fontsize=12, fontweight='bold', color='#333333')\n",
    "    plt.yticks(fontsize=12, fontweight='bold', color='#333333')\n",
    "\n",
    "    # Save the plot as PNG\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Output Forecast/Univariate/{attack_title}_{a+1}.png\")\n",
    "    plt.show()\n",
    "    plt.gcf()\n",
    "\n",
    "\n",
    "data = pd.read_csv('Input/InputData.csv',header=0)\n",
    "\n",
    "attack_list=['Mentions-IoT Device Attack','Cryptojacking-ALL','Mentions-WannaCry Ransomware', 'Mentions-Data Poisoning','Mentions-Deepfake','Mentions-Adversarial Attack','Ransomware-ALL']\n",
    "for attack in attack_list:\n",
    "\n",
    "  input_data = data[[attack]]\n",
    "  attack_title=attack[:-4]\n",
    "\n",
    "  if 'Mentions' in attack:\n",
    "    attack_title = attack[9:]\n",
    "  \n",
    "  param_distributions = {\n",
    "    'alpha': [0.05, 0.1,0.15,0.2,0.3,0.5,0.7,1],  \n",
    "    'beta': [0.4,0.6, 0.7,0.75,0.8,1],  \n",
    "    'unit': [[32], [64], [100], [128], [256], [32,16], [64,32], [128,64], [256,128]], \n",
    "    'epoch': [50, 100, 150, 200],  \n",
    "    'lr': loguniform(1e-4, 1e-3),  \n",
    "    'rdo': uniform(0.1, 0.2),  \n",
    "    'n_features': [input_data.shape[1]], \n",
    "    'attack': [attack],  \n",
    "    'l2_reg': uniform(0.008, 0.04)\n",
    "  }   \n",
    "\n",
    "  input_data = input_data.values\n",
    "  best_smape = 0.999\n",
    "\n",
    "\n",
    "  #----------------------Model Evaluation part-----------------------------------------------------------\n",
    "\n",
    "  ## Model Evaluation with 60 combinations of hyperparam\n",
    "  best_model,best_param, best_mape = model_evaluation(input_data,attack,param_distributions,attack_title,60,best_smape)\n",
    "  with open('Best_Hyperparams/Univariate/'+attack_title.replace('/','_')+'.txt', 'w') as f:\n",
    "        f.write('SMAPE: '+str(best_mape))\n",
    "        f.write('\\nHyperparams: '+str(best_param))\n",
    "        f.close()\n",
    "\n",
    "\n",
    "\n",
    "#------------ This is the part where the results can be reproduced by commenting above evaluation part and just executing below forecast method-------------------------\n",
    "\n",
    "\n",
    "  # Reading best hyperparam from the file\n",
    "  best_hyperparams=[]\n",
    "  with open('Best_Hyperparams/Univariate/'+attack_title.replace('/','_')+'.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    smape_value = float(lines[0].split(\":\")[1].strip())  # Extract SMAPE value\n",
    "    hyperparams_string = \":\".join(lines[1:]).split(\":\", 1)[1].strip()  # Join all the lines after SMAPE and extract the dictionary\n",
    "    best_hyperparams = ast.literal_eval(hyperparams_string) \n",
    "  \n",
    "  #This is the part where the results can be reproduced by commenting above evaluation part and just doing forecast\n",
    "  repeat=3\n",
    "  forecast(input_data,best_hyperparams,attack_title ,repeat)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
