{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import tensorflow_probability as tfp\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "from numpy import split\n",
    "import random\n",
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n",
    "from scipy.optimize import minimize\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import numpy\n",
    "from tqdm import tqdm_notebook\n",
    "from keras.callbacks import EarlyStopping\n",
    "from scipy.stats import loguniform, uniform, randint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import array\n",
    "#from itertools import product\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Layer\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed,BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from numpy.random import seed\n",
    "from scipy import stats\n",
    "from IPython.display import clear_output\n",
    "import statistics\n",
    "import ast\n",
    "\n",
    "import keras.backend as K\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from tensorflow.keras import Input, Model\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def model_based_feature_selection(df, target_column, num_features,attack_title):\n",
    "    # Separate target and predictors\n",
    "    target = df[target_column].values\n",
    "    predictors = df.drop(columns=[target_column]).values\n",
    "    feature_names = df.drop(columns=[target_column]).columns.tolist()\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=500, random_state=42)\n",
    "    model.fit(predictors, target)\n",
    "\n",
    "    # Print feature importances for debugging\n",
    "    feature_importances = model.feature_importances_\n",
    "    print(f\"Feature importances: {feature_importances}\")\n",
    "\n",
    "    # Get the indices of the top features based on importance\n",
    "    indices = np.argsort(feature_importances)[::-1]\n",
    "    top_indices = indices[:num_features]  # Indices of the top features\n",
    "\n",
    "    # Select the features using the indices\n",
    "    selected_data = np.zeros((predictors.shape[0], num_features + 1))  # Prepare the array with the correct shape\n",
    "    selected_data[:, 0] = target  # First column is the target feature\n",
    "    selected_data[:, 1:] = predictors[:, top_indices]  # Remaining columns are the selected features\n",
    "\n",
    "    # Get the feature names of the top selected features\n",
    "    selected_feature_names = [feature_names[i] for i in top_indices]\n",
    "\n",
    "    # Plot feature importances for all features\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(feature_importances)))  # Different colors for each bar\n",
    "    ax.bar(range(len(feature_importances)), feature_importances[indices], color=colors, width=0.5)\n",
    "    ax.set_ylabel('Feature Importance')\n",
    "    ax.set_xlabel('Features')\n",
    "    ax.set_title(f'{attack_title} - Feature Importance')\n",
    "    ax.set_ylim(0, 1)  # Limit y-axis to 1\n",
    "    ax.set_xticks(range(len(feature_importances)))  # Ensure x-ticks match the number of bars\n",
    "    ax.set_xticklabels([feature_names[i] for i in indices], rotation=45)  # Rotate x labels for better visibility\n",
    "    plt.tight_layout()  # Adjust layout to prevent clipping\n",
    "    plt.show()\n",
    "\n",
    "    return selected_data, selected_feature_names\n",
    "\n",
    "\n",
    "\n",
    "# evaluation mertic (symmteric mean absolute percentage error)\n",
    "def smape(yTrue,yPred):\n",
    "  smape=0\n",
    "\n",
    "  for i in range(len(yTrue)):\n",
    "    smape+= abs(yTrue[i]-yPred[i])/ (abs(yTrue[i])+abs(yPred[i]))\n",
    "  smape/=len(yTrue)\n",
    "\n",
    "  return smape\n",
    "\n",
    "\n",
    "# Dynamic Penalty Function - Calculate IQR-based penalty\n",
    "def calculate_dynamic_iqr_penalty(actual, mean_predictions):\n",
    "    \"\"\"Calculate dynamic penalty based on IQR.\"\"\"\n",
    "    iqr_actual = np.percentile(actual, 75) - np.percentile(actual, 25)\n",
    "    iqr_pred = np.percentile(mean_predictions, 75) - np.percentile(mean_predictions, 25)\n",
    "    \n",
    "    # Define the penalty dynamically based on the difference in IQRs\n",
    "    if iqr_pred < iqr_actual:\n",
    "        iqr_ratio = iqr_pred / iqr_actual\n",
    "        penalty = (1 - iqr_ratio) * 0.3  # Penalty is proportional to IQR difference\n",
    "        print(f'**** PENALIZED: +{penalty:.4f} ******')\n",
    "        return penalty  # Return only the penalty\n",
    "    else:\n",
    "        return 0  # No penalty if predicted IQR is sufficient\n",
    "\n",
    "\n",
    "def exponential_smoothing(series, alpha):\n",
    "    smoothed_series = [series[0]]\n",
    "    for i in range(1, len(series)):\n",
    "        smoothed_series.append(alpha * series[i] + (1 - alpha) * smoothed_series[i-1])\n",
    "    return smoothed_series\n",
    "\n",
    "def des_smoothing(series, alpha, beta):\n",
    "    \"\"\"\n",
    "    Applies Double Exponential Smoothing (DES) to a time series for both smoothing and forecasting.\n",
    "\n",
    "    Parameters:\n",
    "    series (list or array-like): The input time series data.\n",
    "    alpha (float): Smoothing factor for the level (between 0 and 1).\n",
    "    beta (float): Smoothing factor for the trend (between 0 and 1).\n",
    "\n",
    "    Returns:\n",
    "    list: The smoothed series with forecasted values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the smoothed series with the first value\n",
    "    smoothed_series = [series[0]]\n",
    "\n",
    "    for i in range(1, len(series) + 1):\n",
    "        if i == 1:\n",
    "            # Initial level and trend setup\n",
    "            level = series[0]\n",
    "            trend = series[1] - series[0]\n",
    "        if i >= len(series):\n",
    "            # If we're forecasting beyond the data, use the last smoothed value\n",
    "            observation = smoothed_series[-1]\n",
    "        else:\n",
    "            # Otherwise, use the actual data\n",
    "            observation = series[i]\n",
    "\n",
    "        # Store the previous level for trend calculation\n",
    "        previous_level = level\n",
    "\n",
    "        # Update the level and trend using the smoothing factors\n",
    "        level = alpha * observation + (1 - alpha) * (level + trend)\n",
    "        trend = beta * (level - previous_level) + (1 - beta) * trend\n",
    "\n",
    "        # Append the new smoothed value (level + trend)\n",
    "        smoothed_series.append(level + trend)\n",
    "\n",
    "    return smoothed_series\n",
    "\n",
    "\n",
    "def determine_optimal_lag(series,max_lags,lags):\n",
    "    \"\"\"\n",
    "    Determine the optimal lag for a time series using ACF.\n",
    "\n",
    "    Parameters:\n",
    "    series (array-like): The input time series data.\n",
    "    max_lags (int): Maximum time window size or lag which is allowed\n",
    "    lags: upto  these lags correlations will be identified\n",
    "    Returns:\n",
    "    int: The optimal lag to use for the model.\n",
    "    \"\"\"\n",
    "    # Calculate ACF and PACF values\n",
    "    acf_values = acf(series, nlags=lags)\n",
    "    # pacf_values = pacf(series, nlags=lags)\n",
    "    \n",
    "    # Determine significance level\n",
    "    significance_level = 1.96 / np.sqrt(len(series))\n",
    "    \n",
    "    # Find the first lag where ACF and PACF drop below the significance level\n",
    "    optimal_lag_acf = np.where(np.abs(acf_values) < significance_level)[0]\n",
    "    # optimal_lag_pacf = np.where(np.abs(pacf_values) < significance_level)[0]\n",
    "    \n",
    "    # Take the first occurrence or default to max_lags if not found\n",
    "    optimal_lag_acf = optimal_lag_acf[0] if len(optimal_lag_acf) > 1 else max_lags\n",
    "    # optimal_lag_pacf = optimal_lag_pacf[0] if len(optimal_lag_pacf) > 1 else max_lags\n",
    "    \n",
    "    # Select the maximum of ACF and PACF lags\n",
    "    optimal_lag = min(optimal_lag_acf, max_lags)\n",
    "    \n",
    "    # Plot ACF and PACF for inspection\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "    plot_acf(series, lags=lags, ax=ax)\n",
    "    ax.set_title('Autocorrelation Function (ACF)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Optimal Lag: {optimal_lag}\")\n",
    "    return optimal_lag\n",
    "\n",
    "\n",
    "\n",
    "def create_multivariate_lagged_dataset(series, n_lags):\n",
    "    \"\"\"\n",
    "    Create a dataset where each time step t uses the previous n_lags time steps of all features as inputs.\n",
    "\n",
    "    Parameters:\n",
    "    series (array-like): The input time series data with multiple features (shape: [num_samples, num_features]).\n",
    "    n_lags (int): The number of previous time steps to use as inputs.\n",
    "\n",
    "    Returns:\n",
    "    X (numpy array): Input features of shape (num_samples, n_lags, num_features).\n",
    "    y (numpy array): Output labels of shape (num_samples, num_features).\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "\n",
    "    # Ensure the series is a numpy array\n",
    "    series = np.array(series)\n",
    "\n",
    "    if series.ndim == 1:\n",
    "      series = series.reshape(-1, 1)\n",
    "\n",
    "    # Loop over the series and create input-output pairs\n",
    "    for i in range(n_lags, len(series[:,0])):\n",
    "        X.append(series[i-n_lags:i, :])  # The previous n_lags time steps for all features\n",
    "        y.append(series[i, :])           # The current time step for all features\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "#Bayesian LSTM\n",
    "class BayesianLSTM(LSTM): #inherits LSTM but set training=True which keeps dropout on during test time.\n",
    "\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(BayesianLSTM, self).__init__(units, **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(BayesianLSTM, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        # Call the parent class LSTM with training set to True\n",
    "        return super(BayesianLSTM, self).call(inputs, training=True, **kwargs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_sequences:\n",
    "            return (input_shape[0], input_shape[1], self.units)\n",
    "        else:\n",
    "            return (input_shape[0], self.units)\n",
    "\n",
    "class InputAttention(Layer):\n",
    "    def __init__(self, n_features, **kwargs):\n",
    "        super(InputAttention, self).__init__(**kwargs)\n",
    "        self.n_features = n_features\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], input_shape[-1]), initializer='glorot_uniform', trainable=True)\n",
    "        self.b = self.add_weight(name='attention_bias', shape=(input_shape[-1],), initializer='zeros', trainable=True)\n",
    "        self.s = self.add_weight(name='importance_score', shape=(input_shape[-1],input_shape[-1]), initializer='glorot_uniform', trainable=True)\n",
    "        super(InputAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Tenor dot multiplication with input and weight on feature level axis\n",
    "        uit = tf.tanh(tf.tensordot(x, self.W, axes=1) + self.b)\n",
    "\n",
    "        # Tensor dot multiplication with score vector and last vector  on feature level axi\n",
    "        ait = tf.tensordot(uit, self.s, axes=[[2],[0]])\n",
    "        ait = tf.nn.softmax(ait, axis=-1)\n",
    "\n",
    "        weighted_input = x * ait\n",
    "        return weighted_input,ait\n",
    "\n",
    "\n",
    "\n",
    "class TemporalAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TemporalAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='temporal_attention_weight', shape=(input_shape[-1], input_shape[-1]), initializer='glorot_uniform', trainable=True)\n",
    "        self.b = self.add_weight(name='temporal_attention_bias', shape=(input_shape[-1],), initializer='zeros', trainable=True)\n",
    "        self.s = self.add_weight(name='importance_score', shape=(input_shape[-1],), initializer='glorot_uniform', trainable=True)\n",
    "        super(TemporalAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        #\n",
    "        uit = tf.tanh(tf.tensordot(x, self.W, axes=1) + self.b)\n",
    "        ait = tf.tensordot(uit, self.s, axes=1) # tenosr dot operation on time step axis level\n",
    "        ait = tf.nn.softmax(ait, axis=1)\n",
    "        ait = tf.expand_dims(ait, axis=-1)\n",
    "        weighted_input = x * ait\n",
    "        return tf.reduce_sum(weighted_input, axis=1),ait\n",
    "\n",
    "\n",
    "# Build and train the model using Dual Stage Attention Mechansim\n",
    "def build_model(data, n_input, layer, unit, epoch, lr, rdo, n_features, l2_reg, attack_title, batchsize):\n",
    "    # Prepare data\n",
    "    train_x, train_y = create_multivariate_lagged_dataset(data, n_input)\n",
    "    \n",
    "    # Define model inputs\n",
    "    inputs = Input(shape=(n_input, n_features))\n",
    " \n",
    "    # Input Attention Mechanism applied to the input features\n",
    "    input_attention, input_attention_scores = InputAttention(n_features)(inputs)\n",
    "\n",
    "    # Add LSTM and Attention Mechanism based on the number of layers\n",
    "    if layer == 1:\n",
    "        # First LSTM Encoder Layer  with L2 Regularization\n",
    "        ec_layer1_output = BayesianLSTM(unit[0], input_shape=(n_input, n_features), \n",
    "                                   recurrent_dropout=rdo, return_sequences=True, \n",
    "                                   activation=keras.layers.LeakyReLU(alpha=0.2),\n",
    "                                   kernel_regularizer=l2(l2_reg))(input_attention)\n",
    "           \n",
    "        # Batch Normalization after LSTM layer\n",
    "        ec_layer1_output = BatchNormalization()(ec_layer1_output)\n",
    "        \n",
    "        # Temporal Attention Mechanism applied to the hidden states\n",
    "        temporal_attention_output, temporal_attention_scores = TemporalAttention()(ec_layer1_output)\n",
    "\n",
    "        repeated_vector = RepeatVector(1)(temporal_attention_output)\n",
    "\n",
    "        # First LSTM Decoder Layer with L2 Regularization\n",
    "        dc_layer1_output = BayesianLSTM(unit[0], activation=keras.layers.LeakyReLU(alpha=0.2), \n",
    "                                   return_sequences=True, recurrent_dropout=rdo,\n",
    "                                   kernel_regularizer=l2(l2_reg))(repeated_vector)\n",
    "        \n",
    "        # Batch Normalization after Decoder LSTM\n",
    "        dc_final_output = BatchNormalization()(dc_layer1_output)\n",
    "        \n",
    "    elif layer == 2:\n",
    "        # First encoder  LSTM Layer with L2 Regularization\n",
    "        ec_layer1_output = BayesianLSTM(unit[0], input_shape=(n_input, n_features), \n",
    "                                   recurrent_dropout=rdo, return_sequences=True, \n",
    "                                   activation=keras.layers.LeakyReLU(alpha=0.2),\n",
    "                                   kernel_regularizer=l2(l2_reg))(input_attention)\n",
    "        \n",
    "        # Batch Normalization after first LSTM\n",
    "        ec_layer1_output = BatchNormalization()(ec_layer1_output)\n",
    "        \n",
    "        # Second encoder LSTM Layer with L2 Regularization\n",
    "        ec_layer2_output = BayesianLSTM(unit[1], activation=keras.layers.LeakyReLU(alpha=0.2), \n",
    "                                   return_sequences=True, recurrent_dropout=rdo,\n",
    "                                   kernel_regularizer=l2(l2_reg))(ec_layer1_output)\n",
    "        \n",
    "        # Batch Normalization after second LSTM\n",
    "        ec_layer2_output = BatchNormalization()(ec_layer2_output)\n",
    "        \n",
    "        # Temporal Attention Mechanism applied to the hidden states\n",
    "        temporal_attention_output, temporal_attention_scores = TemporalAttention()(ec_layer2_output)\n",
    "        repeated_vector = RepeatVector(1)(temporal_attention_output)\n",
    "\n",
    "        # Decoder LSTM Layers with L2 Regularization\n",
    "        dc_layer1_output = BayesianLSTM(unit[1], activation=keras.layers.LeakyReLU(alpha=0.2), \n",
    "                                   return_sequences=True, recurrent_dropout=rdo,\n",
    "                                   kernel_regularizer=l2(l2_reg))(repeated_vector)\n",
    "        \n",
    "        # Batch Normalization after first Decoder LSTM\n",
    "        dc_layer1_output = BatchNormalization()(dc_layer1_output)\n",
    "        \n",
    "        dc_layer2_output = BayesianLSTM(unit[0], activation=keras.layers.LeakyReLU(alpha=0.2), \n",
    "                                   return_sequences=True, recurrent_dropout=rdo,\n",
    "                                   kernel_regularizer=l2(l2_reg))(dc_layer1_output)\n",
    "        \n",
    "        # Batch Normalization after second Decoder LSTM\n",
    "        dc_final_output = BatchNormalization()(dc_layer2_output)\n",
    "\n",
    "    # Output Layers with L2 Regularization (Common for both conditions)\n",
    "    outputs = TimeDistributed(Dense(100, activation=keras.layers.LeakyReLU(alpha=0.2), \n",
    "                                    kernel_regularizer=l2(l2_reg)))(dc_final_output)\n",
    "    final_output = TimeDistributed(Dense(n_features, kernel_regularizer=l2(l2_reg)))(outputs)\n",
    "\n",
    "    # Create the encoder decoder model with input and temporal attention mechnaism with single input and multiple outputs\n",
    "    model = Model(inputs=inputs, outputs=[final_output, input_attention_scores, temporal_attention_scores])\n",
    "\n",
    "    # Compile the model\n",
    "    opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=opt, loss=['mse',None,None], metrics=['mape',None,None])\n",
    "\n",
    "    # Early stopping and ReduceLROnPlateau callbacks\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "    # Fit the model\n",
    "    print(train_x.shape)\n",
    "    train_history = model.fit(train_x, train_y, epochs=epoch, batch_size=batchsize, \n",
    "                              validation_split=0.2, verbose=0, callbacks=[es, reduce_lr])\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.plot(train_history.history['loss'], label='Training Loss')\n",
    "    plt.plot(train_history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"Training_Results/Multivariate/{attack_title}.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Return the model\n",
    "    return model\n",
    "\n",
    "def scale_data(data):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    #Normalising the columns\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    return scaled_data\n",
    "\n",
    "def smooth_and_plot_data(data, alpha,attack, n_features,features,beta=1):\n",
    "    # Split the data into training and testing\n",
    "    featureList=[]\n",
    "    mention_as_feature=False\n",
    "    plt.clf()\n",
    "    if not alpha==1 and not beta==1:\n",
    "        smoothed= des_smoothing(data[:,0], alpha,beta)\n",
    "        smoothed=smoothed[0:-1] #remove the one future step forecast from des\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(data[:, 0], label=\"Actual\", color='blue', linestyle='-', linewidth=2)\n",
    "        plt.plot(smoothed, label=f\"Smoothed (α={alpha}, β={beta})\", color=\"red\", linestyle='--', linewidth=2)\n",
    "        plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "        plt.title(f\"Double Exponential Smoothing - {attack}\", fontsize=16, fontweight='bold')\n",
    "        plt.xlabel(\"Month\", fontsize=12)\n",
    "        if 'Mentions-' in attack:\n",
    "            plt.ylabel(\"No of mentions\")\n",
    "        else:\n",
    "            mention_as_feature= False\n",
    "            plt.ylabel(\"No of incidents\")\n",
    "        # Highlighting the max point in the smoothed data\n",
    "        max_val, min_val = np.max(smoothed), np.min(smoothed)\n",
    "        max_index, min_index = np.argmax(smoothed), np.argmin(smoothed)\n",
    "        plt.scatter(max_index, max_val, color='green', s=50, label=f'Max Value: {max_val:.2f}')\n",
    "        plt.scatter(min_index, min_val, color='pink', s=50, label=f'Min Value: {min_val:.2f}')\n",
    "        plt.tight_layout()\n",
    "        plt.legend(loc=\"best\", fontsize=12)\n",
    "        plt.show() #plot the data and smoothed data\n",
    "        featureList.append(smoothed)\n",
    "\n",
    "        if mention_as_feature:\n",
    "            smoothed= des_smoothing(data[:,1], alpha,beta)\n",
    "            smoothed=smoothed[0:-1] #remove the one future step forecast from des\n",
    "            smoothed = np.array(smoothed)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "\n",
    "            # Plot Actual vs Smoothed\n",
    "            plt.plot(data[:, 1], label=\"Actual\", color='blue', linestyle='-', linewidth=2)\n",
    "            plt.plot(smoothed, label=f\"Smoothed (α={alpha}, β={beta})\", color=\"red\", linestyle='--', linewidth=2)\n",
    "            max_val, min_val = np.max(smoothed), np.min(smoothed)\n",
    "            max_index, min_index = np.argmax(smoothed), np.argmin(smoothed)\n",
    "            plt.scatter(max_index, max_val, color='green', s=50, label=f'Max Value: {max_val:.2f}')\n",
    "            plt.scatter(min_index, min_val, color='pink', s=50, label=f'Min Value: {min_val:.2f}')   \n",
    "\n",
    "            # Customizations\n",
    "            plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "            plt.title(f\"Double Exponential Smoothing - {attack[:-4]}\", fontsize=16, fontweight='bold')\n",
    "            plt.xlabel(\"Month\", fontsize=12)\n",
    "            plt.ylabel(\"No of Mentions\", fontsize=12)\n",
    "            plt.tight_layout()\n",
    "            plt.legend(loc=\"best\", fontsize=12)\n",
    "\n",
    "            # Show plot\n",
    "            plt.show()\n",
    "            featureList.append(smoothed)\n",
    "            mention_as_feature= False\n",
    "        for index,feature in enumerate(features,start=len(featureList)):\n",
    "            smoothed= des_smoothing(data[:,index], alpha,beta)\n",
    "            smoothed=smoothed[0:-1] #remove the one future step forecast from des\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.plot(data[:, index], label=\"Actual\", color='blue', linestyle='-', linewidth=2)\n",
    "            plt.plot(smoothed, label=f\"Smoothed (α={alpha}, β={beta})\", color=\"red\", linestyle='--', linewidth=2)\n",
    "            max_val, min_val = np.max(smoothed), np.min(smoothed)\n",
    "            max_index, min_index = np.argmax(smoothed), np.argmin(smoothed)\n",
    "            plt.scatter(max_index, max_val, color='green', s=50, label=f'Max Value: {max_val:.2f}')\n",
    "            plt.scatter(min_index, min_val, color='pink', s=50, label=f'Min Value: {min_val:.2f}')\n",
    "            plt.title(f\"Double Exponential Smoothing - {feature}\", fontsize=16, fontweight='bold')\n",
    "            plt.xlabel(\"Month\", fontsize=12)\n",
    "            plt.ylabel(\"Number\", fontsize=12)\n",
    "            plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "            plt.legend(loc=\"best\", fontsize=12)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            featureList.append(smoothed)\n",
    "    elif not alpha==1 and beta==1:\n",
    "        smoothed= exponential_smoothing(data[:,0], alpha)\n",
    "        smoothed=smoothed[0:-1] #remove the one future step forecast from des\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(data[:, 0], label=\"Actual\", color='blue', linestyle='-', linewidth=2)\n",
    "        plt.plot(smoothed, label=f\"Smoothed (α={alpha}, β={beta})\", color=\"red\", linestyle='--', linewidth=2)\n",
    "        plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "        plt.title(f\"Exponential Smoothing - {attack}\", fontsize=16, fontweight='bold')\n",
    "        plt.xlabel(\"Month\", fontsize=12)\n",
    "        if 'Mentions-' in attack:\n",
    "            plt.ylabel(\"No of mentions\")\n",
    "        else:\n",
    "            mention_as_feature= True\n",
    "            plt.ylabel(\"No of incidents\")\n",
    "        # Highlighting the max point in the smoothed data\n",
    "        max_val, min_val = np.max(smoothed), np.min(smoothed)\n",
    "        max_index, min_index = np.argmax(smoothed), np.argmin(smoothed)\n",
    "        plt.scatter(max_index, max_val, color='green', s=50, label=f'Max Value: {max_val:.2f}')\n",
    "        plt.scatter(min_index, min_val, color='pink', s=50, label=f'Min Value: {min_val:.2f}')\n",
    "        plt.tight_layout()\n",
    "        plt.legend(loc=\"best\", fontsize=12)\n",
    "        plt.show() #plot the data and smoothed data\n",
    "        featureList.append(smoothed)\n",
    "\n",
    "        if mention_as_feature:\n",
    "            smoothed= exponential_smoothing(data[:,1], alpha)\n",
    "            smoothed=smoothed[0:-1] #remove the one future step forecast from des\n",
    "            smoothed = np.array(smoothed)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "\n",
    "            # Plot Actual vs Smoothed\n",
    "            plt.plot(data[:, 1], label=\"Actual\", color='blue', linestyle='-', linewidth=2)\n",
    "            plt.plot(smoothed, label=f\"Smoothed (α={alpha}, β={beta})\", color=\"red\", linestyle='--', linewidth=2)\n",
    "            max_val, min_val = np.max(smoothed), np.min(smoothed)\n",
    "            max_index, min_index = np.argmax(smoothed), np.argmin(smoothed)\n",
    "            plt.scatter(max_index, max_val, color='green', s=50, label=f'Max Value: {max_val:.2f}')\n",
    "            plt.scatter(min_index, min_val, color='pink', s=50, label=f'Min Value: {min_val:.2f}')   \n",
    "\n",
    "            # Customizations\n",
    "            plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "            plt.title(f\"Exponential Smoothing - {attack[:-4]}\", fontsize=16, fontweight='bold')\n",
    "            plt.xlabel(\"Month\", fontsize=12)\n",
    "            plt.ylabel(\"No of Mentions\", fontsize=12)\n",
    "            plt.tight_layout()\n",
    "            plt.legend(loc=\"best\", fontsize=12)\n",
    "\n",
    "            # Show plot\n",
    "            plt.show()\n",
    "            featureList.append(smoothed)\n",
    "            mention_as_feature= False\n",
    "        for index,feature in enumerate(features,start=len(featureList)):\n",
    "            smoothed= exponential_smoothing(data[:,index], alpha)\n",
    "            smoothed=smoothed[0:-1] #remove the one future step forecast from des\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.plot(data[:, index], label=\"Actual\", color='blue', linestyle='-', linewidth=2)\n",
    "            plt.plot(smoothed, label=f\"Smoothed (α={alpha}, β={beta})\", color=\"red\", linestyle='--', linewidth=2)\n",
    "            max_val, min_val = np.max(smoothed), np.min(smoothed)\n",
    "            max_index, min_index = np.argmax(smoothed), np.argmin(smoothed)\n",
    "            plt.scatter(max_index, max_val, color='green', s=50, label=f'Max Value: {max_val:.2f}')\n",
    "            plt.scatter(min_index, min_val, color='pink', s=50, label=f'Min Value: {min_val:.2f}')\n",
    "            plt.title(f\"Exponential Smoothing - {feature}\", fontsize=16, fontweight='bold')\n",
    "            plt.xlabel(\"Month\", fontsize=12)\n",
    "            plt.ylabel(\"Number\", fontsize=12)\n",
    "            plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "            plt.legend(loc=\"best\", fontsize=12)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            featureList.append(smoothed)\n",
    "    else:\n",
    "        return data    \n",
    "    featureList=numpy.transpose(featureList)\n",
    "    prepared_data=numpy.array(featureList).reshape(-1,n_features)\n",
    "    return prepared_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_test_split(data, n_input):\n",
    "  split_position = len(data) - (36 + n_input)\n",
    "  if split_position <= 0:\n",
    "    raise ValueError(\"n_input is too large for the dataset size.\")\n",
    "  train, test = data[:split_position], data[split_position:]\n",
    "  return train,test\n",
    "a= 0\n",
    "\n",
    "def model_evaluation(data, attack, param_distributions, attack_title,n_iter,selected_features,best_smape):\n",
    "    \"\"\"\n",
    "    Uses RandomizedSearchCV to evaluate the model on different hyperparameter combination\n",
    "\n",
    "    Parameters:\n",
    "    data: The input data for evaluation after model based feature selection\n",
    "    attack: The target column to focus on.\n",
    "    param_distributions: Dictionary of hyperparameter distributions for RandomizedSearchCV.\n",
    "    n_iter: Number of random selections for RandomizedSearchCV.\n",
    "    selected_features: Selected feature names in a list\n",
    "    best_smape: Best Smape value to start with and then later on changed based on result\n",
    "\n",
    "    Returns:\n",
    "    best_model: The best performing model based on the mean SMAPE.\n",
    "    best_params: The best hyperparameters.\n",
    "    best_mape: The lowest average SMAPE achieved.\n",
    "    \"\"\"\n",
    "\n",
    "    class BayesianLSTMEstimator(BaseEstimator, RegressorMixin):\n",
    "        def __init__(self, alpha, beta, n_input,unit, epoch, lr, rdo, n_features, attack,l2_reg,batch_size):\n",
    "            self.alpha = alpha\n",
    "            self.beta = beta\n",
    "            self.n_input = None\n",
    "            self.unit = unit\n",
    "            self.epoch = epoch\n",
    "            self.lr = lr\n",
    "            self.rdo = rdo\n",
    "            self.n_features = n_features\n",
    "            self.attack = attack\n",
    "            self.train,self.test=None,None\n",
    "            self.scaler=None\n",
    "            self.scaled_data = None\n",
    "            self.relevant_scaled_data=None\n",
    "            self.l2_reg = l2_reg\n",
    "            self.batch_size  = batch_size\n",
    "\n",
    "        def fit(self, X, y=None):\n",
    "            global a\n",
    "            seed(a+1)\n",
    "            tf.random.set_seed(a+1)\n",
    "\n",
    "\n",
    "            print(f'Selected Parameters: {self.get_params()}')\n",
    "            print('--------------------------------------------------------------------------------')\n",
    "            print(f\"Smoothing with alpha={self.alpha}, beta={self.beta}\")\n",
    "            smoothed_data = smooth_and_plot_data(X, self.alpha, self.attack, self.n_features, selected_features,self.beta)\n",
    "            self.scaled_data = scale_data(smoothed_data)\n",
    "            print('data shape=',self.scaled_data.shape)\n",
    "            e_index=0 #emerging index\n",
    "            for i in range(len(self.scaled_data[:,0])-2):\n",
    "                if self.scaled_data[i,0]>0 and self.scaled_data[i+1,0]>0 and self.scaled_data[i+2,0]>0:\n",
    "                    e_index=i\n",
    "                    break\n",
    "            print('Emerging Index=',e_index)\n",
    "\n",
    "            self.relevant_scaled_data=self.scaled_data[e_index:,]\n",
    "            print('Length of data after cut=',len(self.relevant_scaled_data[:,0]))\n",
    "            max_lags = (len(self.relevant_scaled_data[:,0])//10)+1\n",
    "            self.n_input = determine_optimal_lag(self.relevant_scaled_data[:,0],max_lags,36)\n",
    "        \n",
    "            self.train, self.test = train_test_split(self.relevant_scaled_data, self.n_input)\n",
    "            # Build and fit the model\n",
    "            self.model = build_model(self.relevant_scaled_data, self.n_input, len(self.unit), self.unit, self.epoch, self.lr, self.rdo, self.n_features, self.l2_reg,attack_title,self.batch_size)\n",
    "            return self\n",
    "\n",
    "        def predict(self, X, y=None):\n",
    "          \"\"\"\n",
    "          Perform multiple iterations of predictions and calculate the mean,\n",
    "          along with lower and upper bounds for uncertainty estimation.\n",
    "          \"\"\"\n",
    "          n_iterations = 10  # Number of iterations for Monte Carlo sampling\n",
    "          all_predictions = []\n",
    "          initial_history = self.test[:self.n_input]  # Initial history for generating future predictions\n",
    "\n",
    "          # Perform n_iterations of forecasting\n",
    "          for _ in range(n_iterations):\n",
    "              history = initial_history.copy()\n",
    "              predictions = []\n",
    "\n",
    "              # Rolling window prediction for the test set\n",
    "              for _ in range(len(self.test) - self.n_input):\n",
    "                  #index 0 because model return attentionn scores as well in output\n",
    "                  y_pred = self.model.predict(history.reshape(1, history.shape[0], history.shape[1]))[0]\n",
    "                  next_value = y_pred[0][0]  # Extract the forecasted value\n",
    "                  predictions.append(next_value)\n",
    "\n",
    "                  # Update the history by appending the new prediction\n",
    "                  history = np.append(history, next_value).reshape(-1, self.n_features)[1:]\n",
    "\n",
    "              all_predictions.append(predictions)\n",
    "\n",
    "          # Convert to numpy array for statistical analysis\n",
    "          all_predictions = np.array(all_predictions)\n",
    "          # Compute the mean, and the 95% confidence intervals (2.5th and 97.5th percentiles)\n",
    "          mean_forecast = np.mean(all_predictions, axis=0)\n",
    "          lower_bound = np.percentile(all_predictions, 2.5, axis=0)\n",
    "          upper_bound = np.percentile(all_predictions, 97.5, axis=0)\n",
    "          return mean_forecast[:,0], lower_bound[:,0], upper_bound[:,0]\n",
    "\n",
    "\n",
    "        def score(self, X, y=None):\n",
    "            \"\"\"\n",
    "            evaluate the model on SMAPE \n",
    "            The calculated SMAPE after adding penalty is returned as the score in negative as it finds the best smape in increasing order of integer.\n",
    "            \"\"\"\n",
    "            global a\n",
    "            a=a+1\n",
    "            print(f\"Selected Parameters: {self.get_params()}\")\n",
    "            mean_predictions, lower_bound, upper_bound = self.predict(X)\n",
    "            smape_score = smape(self.test[self.n_input:, 0], mean_predictions)\n",
    "            # Apply dynamic IQR penalty\n",
    "            penalty = calculate_dynamic_iqr_penalty(self.test[self.n_input:, 0], mean_predictions)\n",
    "            final_score = smape_score + penalty  # Apply the penalty to SMAPE\n",
    "            global best_smape            \n",
    "            self.plot_predictions(mean_predictions, lower_bound, upper_bound,final_score,best_smape)\n",
    "            print(f\"Initial SMAPE = {smape_score}, Penalty = {penalty},  Corrected SMAPE= {final_score}\")\n",
    "\n",
    "            if final_score < best_smape:\n",
    "                best_smape = final_score\n",
    "            print(f'Best SMAPE uptill now: {best_smape}')               \n",
    "            print(f\"Current Iteration SMAPE = {final_score}\")\n",
    "            \n",
    "            print(f'-----------End of Iteration {a+1}--------------------------')\n",
    "            return -final_score  # Negative because RandomizedSearchCV maximizes the score, but we want to minimize SMAPE\n",
    "\n",
    "        def plot_predictions(self, mean_predictions, lower_bound, upper_bound,smape,best_smape):\n",
    "            \"\"\"\n",
    "            Plot actual values, mean predictions, and 95% confidence intervals for the current repeat.\n",
    "\n",
    "            Parameters:\n",
    "            mean_predictions: Mean predictions for the current repeat.\n",
    "            lower_bound: Lower bound of the 95% confidence interval.\n",
    "            upper_bound: Upper bound of the 95% confidence interval.\n",
    "            smape: current smape\n",
    "            best_smape: best  smape until now\n",
    "            \"\"\"\n",
    "            # mean_predictions = self.scaler.inverse_transform(mean_predictions.reshape(-1,3)).flatten()\n",
    "            actual = self.test[self.n_input:,0]\n",
    "            # lower_bound = self.scaler.inverse_transform(lower_bound.reshape(-1,3)).flatten()\n",
    "            # upper_bound = self.scaler.inverse_transform(upper_bound.reshape(-1,3)).flatten()\n",
    "            x = range(1, len(mean_predictions) + 1)\n",
    "            plt.figure(figsize=(10, 6), facecolor='white')\n",
    "            plt.plot(x, actual, label='Actual', color='#1f77b4', linestyle='-', linewidth=2.5, marker='o', markersize=6, markerfacecolor='blue')\n",
    "            plt.plot(x, mean_predictions, label='Predicted', color='#ff7f0e', linestyle='--', linewidth=2.5, marker='D', markersize=6, markerfacecolor='orange')\n",
    "            plt.fill_between(x, lower_bound, upper_bound, color='lightgreen', alpha=0.3, label='95% Confidence Interval')\n",
    "            plt.grid(True, which='both', color='gray', linestyle='--', linewidth=0.6, alpha=0.7)\n",
    "            plt.legend(loc=\"best\", fontsize=12, fancybox=True, framealpha=1, shadow=True, borderpad=1)\n",
    "            plt.title(f'{attack} (Multivariate), SMAPE: {smape}', fontsize=18, fontweight='bold', color='#2c3e50', pad=20)\n",
    "            plt.xlabel(\"Months\", fontsize=14, fontweight='bold', color='#34495e')\n",
    "            plt.ylabel(\"Number of Incidents\", fontsize=14, fontweight='bold', color='#34495e')\n",
    "            plt.xticks(fontsize=12, fontweight='bold', color='#2c3e50')\n",
    "            plt.yticks(fontsize=12, fontweight='bold', color='#2c3e50')\n",
    "            plt.tight_layout()\n",
    "            if smape < best_smape:\n",
    "                plt.savefig(f\"Evaluation Result/Multivariate/{attack_title}\"+\".png\")\n",
    "            plt.show()\n",
    "            plt.gcf()\n",
    "\n",
    "    dummy_y = np.zeros(len(data))\n",
    "\n",
    "    # Initialize RandomizedSearchCV\n",
    "    model = BayesianLSTMEstimator(alpha=None, beta=None, unit=None,epoch=None,n_input=None, lr=None, rdo=None, n_features=None, attack=None,l2_reg=None,batch_size=None)\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=n_iter,\n",
    "        cv=[(slice(None), slice(None))],  # No actual CV, just one pass\n",
    "    )\n",
    "\n",
    "    # Fit and find the best model\n",
    "    random_search.fit(data, dummy_y)\n",
    "\n",
    "    best_model = random_search.best_estimator_  # Best model based on the search\n",
    "    best_params = random_search.best_params_\n",
    "    best_mape = -random_search.best_score_  # Convert back to positive SMAPE\n",
    " \n",
    "    print(\"Best Parameters: \", best_params)\n",
    "    print(\"Best Average SMAPE: \", best_mape)\n",
    "\n",
    "    return best_model, best_params, best_mape\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def forecast(data, selected_features, best_params, attack,attack_title, repeat):\n",
    "    \"\"\"\n",
    "    Forecast the next 36 months using the best model from RandomizedSearchCV.\n",
    "    Perform 10 Monte Carlo iterations to account for uncertainty and calculate the mean forecast and confidence intervals.\n",
    "\n",
    "    Parameters:\n",
    "    - data: iNPUT DATA\n",
    "    - Selected_Features: Features selected from the Random Forest Estimator\n",
    "    - best_params: Best hyperparameters returned from RandomizedSearchCV.\n",
    "    - attack_title: Title of the plot.\n",
    "    - repeat: No of Iterations of foreacast\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    alpha = float(best_params['alpha'])\n",
    "    beta = float(best_params['beta'])\n",
    "    unit = [int(x) for x in best_params['unit']]\n",
    "    epoch = int(best_params['epoch'])\n",
    "    lr = float(best_params['lr'])\n",
    "    rdo = float(best_params['rdo'])\n",
    "    n_features = int(best_params['n_features'])\n",
    "    attack = best_params['attack']\n",
    "    l2_reg = float(best_params['l2_reg'])\n",
    "    batch_size = int(best_params['batch_size'])\n",
    "\n",
    "    # Initialize accumulators for Input and Temporal attention\n",
    "    input_attention_accumulator = []\n",
    "    temporal_attention_accumulator = []\n",
    "\n",
    "    for a in range(repeat):\n",
    "        seed(a)\n",
    "        tf.random.set_seed(a)\n",
    "        all_predictions = []\n",
    "        input_attention_all_samples = []\n",
    "        temporal_attention_all_samples = []\n",
    "\n",
    "        smoothed_data = smooth_and_plot_data(data, alpha, attack, n_features, selected_features, beta)\n",
    "        scaled_data = scale_data(smoothed_data)\n",
    "\n",
    "        print('data shape=', scaled_data.shape)\n",
    "        index = 0  # emerging index\n",
    "        for i in range(len(scaled_data[:, 0]) - 2):\n",
    "            if scaled_data[i, 0] > 0 and scaled_data[i + 1, 0] > 0 and scaled_data[i + 2, 0] > 0:\n",
    "                index = i\n",
    "                break\n",
    "        print('Emerging Index =', index)\n",
    "\n",
    "        relevant_scaled_data = scaled_data[index:,]\n",
    "        print('Length of data after cut =', len(relevant_scaled_data[:, 0]))\n",
    "        max_lags = (len(relevant_scaled_data[:, 0]) // 10) + 1\n",
    "        n_input = determine_optimal_lag(relevant_scaled_data[:, 0], max_lags, 36)\n",
    "        model = build_model(relevant_scaled_data, n_input, len(unit), unit, epoch, lr, rdo, n_features, l2_reg, attack_title, batch_size)\n",
    "\n",
    "        # Perform 10 Monte Carlo iterations of forecasting\n",
    "        for iteration in range(10):\n",
    "            history = relevant_scaled_data[-n_input:].copy()  # Start with the last n_input historical data points\n",
    "            predictions = []\n",
    "            input_attention_per_iteration = []\n",
    "            temporal_attention_per_iteration = []\n",
    "\n",
    "            # Generate predictions for the next 36 months\n",
    "            for _ in range(36):\n",
    "                y_pred, input_attention_scores, temporal_attention_scores = model.predict(history.reshape(1, history.shape[0], history.shape[1]))\n",
    "                next_value = y_pred[0][0]  # Extract the forecasted value\n",
    "                predictions.append(next_value)\n",
    "\n",
    "                # Collect InputAttention scores\n",
    "                input_attention_avg_features = np.mean(input_attention_scores[0], axis=0)  \n",
    "                input_attention_per_iteration.append(input_attention_avg_features)\n",
    "\n",
    "                # Collect TemporalAttention scores\n",
    "                temporal_attention_per_iteration.append(temporal_attention_scores[0].squeeze())\n",
    "\n",
    "                # Update the history by appending the new prediction and removing the oldest value\n",
    "                history = np.append(history, next_value).reshape(-1, n_features)[1:]\n",
    "\n",
    "            # Store predictions for this iteration\n",
    "            all_predictions.append(predictions)\n",
    "\n",
    "            # Average InputAttention across samples (i.e., across all predictions)\n",
    "            input_attention_all_samples.append(np.mean(np.array(input_attention_per_iteration), axis=0))\n",
    "\n",
    "            # Average TemporalAttention across samples (i.e., across all predictions)\n",
    "            temporal_attention_all_samples.append(np.mean(np.array(temporal_attention_per_iteration), axis=0))\n",
    "\n",
    "        # Now plot the forecast along with the confidence intervals\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        mean_forecasts = np.mean(all_predictions, axis=0)\n",
    "        lower_bound = np.percentile(all_predictions, 2.5, axis=0)\n",
    "        upper_bound = np.percentile(all_predictions, 97.5, axis=0)\n",
    "\n",
    "        # Forecast Plot\n",
    "        plot_forecast(mean_forecasts[:,0],lower_bound[:,0], upper_bound[:,0], attack_title, scaled_data[:, 0], a)\n",
    "\n",
    "        # Accumulate the averages across all iterations\n",
    "        input_attention_accumulator.append(np.mean(input_attention_all_samples, axis=0))\n",
    "        temporal_attention_accumulator.append(np.mean(temporal_attention_all_samples, axis=0))\n",
    "\n",
    "    # Final averages for InputAttention and TemporalAttention across all 10 iterations\n",
    "    final_input_attention_avg = np.mean(np.array(input_attention_accumulator), axis=0)\n",
    "    final_temporal_attention_avg = np.mean(np.array(temporal_attention_accumulator), axis=0)\n",
    "\n",
    "    features=[attack] + selected_features\n",
    "    plot_attention_scores(final_input_attention_avg,final_temporal_attention_avg,features)\n",
    "\n",
    "    #Save the forecast results to a text file\n",
    "    with open(f'Output Forecast/Multivariate/{attack_title.replace(\"/\", \"_\")}_forecast_/{a}.txt', 'w') as f:\n",
    "        f.write(f\"Mean Forecast: {mean_forecasts[:,0].tolist()}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_forecast(mean_forecasts, lower_bound, upper_bound, attack_title, historical_actual_data,a):\n",
    "    \"\"\"\n",
    "    Plot the forecast with 95% confidence intervals, including historical data,\n",
    "    from July 2011 to March 2024, followed by the forecast from April 2024 to March 2027.\n",
    "    The x-axis will display yearly labels based on the number of data points.\n",
    "\n",
    "    Parameters:\n",
    "    - mean_forecasts: Mean forecasts for the next 36 months (April 2024 to March 2027).\n",
    "    - lower_bound: Lower bound of the 95% confidence interval for the forecasts.\n",
    "    - upper_bound: Upper bound of the 95% confidence interval for the forecasts.\n",
    "    - attack_title: Title of the plot.\n",
    "    - historical_data: Historical data (actual data) for the period from July 2011 to March 2024.\n",
    "    - a: seed\n",
    "    \"\"\"\n",
    "    total_data_points = len(historical_actual_data) + len(mean_forecasts)\n",
    "\n",
    "    # Generate year labels starting from 2012, placed at every 12th tick (i.e., 9, 21, 33,...)\n",
    "    years = ['2012'] + [str(2012 + (i // 12)) for i in range(12, total_data_points - 8, 12)]\n",
    "    print(years)\n",
    "\n",
    "    # Calculate the tick locations: starting from tick 9, every 12 ticks\n",
    "    ticks = range(9, total_data_points+1, 12)\n",
    "\n",
    "    # Set up the figure\n",
    "    plt.figure(figsize=(14, 7), facecolor='white')\n",
    "\n",
    "    # Plot historical data (July 2011 - March 2024)\n",
    "    plt.plot(range(len(historical_actual_data)), historical_actual_data, label='Historical Data', color='royalblue', linestyle='-', linewidth=2)\n",
    "\n",
    "    plt.plot(range(len(historical_actual_data) - 1, total_data_points), [historical_actual_data[-1]] + list(mean_forecasts), label='Forecast', color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "    # Fill the 95% confidence interval for the forecasted data\n",
    "    plt.fill_between(range(len(historical_actual_data), total_data_points), lower_bound, upper_bound, color='lightgreen', alpha=0.3, label='95% Confidence Interval')\n",
    "\n",
    "    # Add grid lines only at the major ticks (where the labels are)\n",
    "    plt.gca().set_xticks(ticks)  # Set major ticks only at labeled positions\n",
    "    plt.grid(True, which='major', color='lightgray', linestyle='--', linewidth=0.8, alpha=0.9)\n",
    "\n",
    "    # Add labels, title, and legend\n",
    "    plt.legend(loc=\"best\", fontsize=12, fancybox=True, framealpha=1, shadow=True, borderpad=1)\n",
    "    plt.title(f'{attack_title} - Historical Data & 36 Months Forecast(Multivariate)', fontsize=18, fontweight='bold', color='#333333', pad=20)\n",
    "    plt.xlabel(\"Year\", fontsize=14, fontweight='bold', color='#333333')\n",
    "    plt.ylabel(\"Number of Incidents\", fontsize=14, fontweight='bold', color='#333333')\n",
    "\n",
    "    # Set x-axis ticks to match the year labels and apply label formatting\n",
    "    plt.xticks(ticks, years, rotation=45, fontsize=12, fontweight='bold', color='#333333')\n",
    "    plt.yticks(fontsize=12, fontweight='bold', color='#333333')\n",
    "\n",
    "    # Save the plot as PNG\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Output Forecast/Multivariate/{attack_title}_{a+1}.png\")\n",
    "    plt.show()\n",
    "    plt.gcf()\n",
    "\n",
    "def plot_attention_scores(input_attention_scores,temporal_attention_scores,features):\n",
    "          \n",
    "    features_str = [str(f) for f in features]\n",
    "    print(features_str)\n",
    "\n",
    "    # Plot Input Attention scores\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(features)))\n",
    "    plt.bar(features_str, input_attention_scores,color=colors, width=0.5)\n",
    "    plt.title(f'Average Input Attention Scores (Feature-wise) - {attack_title}')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Average Attention Score')\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels if needed\n",
    "    plt.tight_layout()  # Adjust layout to avoid clipping\n",
    "\n",
    "    plt.show()\n",
    "    # Plot Temporal Attention scores\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(range(len(temporal_attention_scores)), temporal_attention_scores, marker='o')\n",
    "    plt.title(f'Average Temporal Attention Scores (Time step-wise) - {attack_title}')\n",
    "    plt.xlabel('Time Step (Hidden State)')\n",
    "    plt.ylabel('Average Attention Score')\n",
    "    plt.show()\n",
    "\n",
    "data = pd.read_csv('Input/InputData.csv',header=0)\n",
    "\n",
    "attack_list=['Mentions-IoT Device Attack','Cryptojacking-ALL','Mentions-WannaCry Ransomware', 'Mentions-Data Poisoning','Mentions-Deepfake','Ransomware-ALL']\n",
    "#\n",
    "\n",
    "for attack in attack_list:\n",
    "\n",
    "#'CONFLICT-ALL','Holidays','Political Anniversary-ALL','Internet User Percentage'\n",
    "  if 'Mentions' in attack:\n",
    "    attack_title = attack[9:]\n",
    "    input_data = data[[attack,'CONFLICT-ALL','Holidays','Political Anniversary-ALL','Internet User Percentage']]\n",
    "  else:\n",
    "    attack_title=attack[:-4]\n",
    "    input_data = data[[attack,'Mentions-'+attack_title,'CONFLICT-ALL','Holidays','Political Anniversary-ALL','Internet User Percentage']]\n",
    "\n",
    "\n",
    "  target_column = input_data.columns[0] \n",
    "  best_features_to_select =2 # Number of best features to select\n",
    "\n",
    "  # Apply model-based feature selection to identify important features\n",
    "  selected_input_data, selected_features = model_based_feature_selection(input_data, target_column, best_features_to_select,attack_title)\n",
    "\n",
    "  # Output the selected features and shape\n",
    "  print(f\"Selected feature indices: {selected_features}\")\n",
    "  print(f\"Shape of selected_data: {selected_input_data.shape}\")\n",
    "\n",
    "  param_distributions = {\n",
    "    'alpha': [0.05, 0.1,0.15,0.2,0.3,0.5,0.7,1],  \n",
    "    'beta': [0.4,0.6, 0.7,0.75,0.8,1],  \n",
    "    'unit': [[24],[32], [64], [100], [128], [256], [32,16], [64,32], [128,64], [256,128]], \n",
    "    'epoch': [50, 100, 150, 200],  \n",
    "    'lr': loguniform(1e-4, 1e-3),  \n",
    "    'rdo': uniform(0.1, 0.3),  \n",
    "    'n_features': [selected_input_data.shape[1]], \n",
    "    'attack': [attack],  \n",
    "    'l2_reg': uniform(0.007, 0.07),  \n",
    "    'batch_size':[2,4]\n",
    "  }   \n",
    "\n",
    "#----------------------Model Evaluation part-----------------------------------------------------------\n",
    "  best_smape = 0.999\n",
    "  best_model,best_param, best_mape = model_evaluation(selected_input_data,attack,param_distributions,attack_title,1,selected_features,best_smape)\n",
    "  with open('Best_Hyperparams/Multivariate/'+attack_title.replace('/','_')+'.txt', 'w') as f:\n",
    "    f.write('SMAPE: '+str(best_mape))\n",
    "    f.write('\\nHyperparams: '+str(best_param))\n",
    "    f.close()\n",
    "\n",
    "\n",
    "#------------ This is the part where the results can be reproduced by commenting above evaluation part and just executing below forecast method-------------------------\n",
    "\n",
    "  ## Reading best hyperparam from the file  \n",
    "  best_hyperparams=[]\n",
    "  with open('Best_Hyperparams/Multivariate/'+attack_title.replace('/','_')+'.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    smape_value = float(lines[0].split(\":\")[1].strip())  # Extract SMAPE value\n",
    "    hyperparams_string = \":\".join(lines[1:]).split(\":\", 1)[1].strip()  # Join all the lines after SMAPE and extract the dictionary\n",
    "    best_hyperparams = ast.literal_eval(hyperparams_string)  # Safely evaluate the dictionary\n",
    "\n",
    "\n",
    "  #Forecasting the threats using the best hyperparam from the file\n",
    "  repeat=3\n",
    "  forecast(selected_input_data,selected_features,best_hyperparams,attack,attack_title ,repeat)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
